/**
 * üéØ VoxBridge Translate Hook - Clean & Efficient  
 * Real-time speech translation with gTTS
 */

import { useState, useRef, useCallback, useEffect } from 'react';
import { SessionStatus, type Language, type TranscriptionEntry } from '../types';
import IndianLanguageAI from '../utils/IndianLanguageAI';

export const useVoxBridgeTranslate = () => {
  const [status, setStatus] = useState<SessionStatus>(SessionStatus.DISCONNECTED);
  const [error, setError] = useState<string | null>(null);
  
  // Load conversation history from localStorage on mount
  const [transcriptionHistory, setTranscriptionHistory] = useState<TranscriptionEntry[]>(() => {
    try {
      const saved = localStorage.getItem('voxbridge_conversation_history');
      if (saved) {
        const parsed = JSON.parse(saved);
        console.log(`üìö Loaded ${parsed.length} previous messages`);
        return parsed;
      }
    } catch (error) {
      console.warn('Failed to load conversation history:', error);
    }
    return [];
  });
  
  const [currentInputTranscription, setCurrentInputTranscription] = useState('');
  const [currentOutputTranscription, setCurrentOutputTranscription] = useState('');

  const recognitionRef = useRef<any>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const analyserNodeRef = useRef<AnalyserNode | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const isProcessingRef = useRef(false);
  const isListeningRef = useRef(false);
  const restartAttemptsRef = useRef(0);
  const maxRestartAttempts = 3; // Maximum restart attempts before recreating recognition
  const maxHistorySize = 100; // Limit conversation history to prevent memory issues
  
  const indianAI = useRef(new IndianLanguageAI());

  // Save conversation history to localStorage whenever it changes (with size limit)
  useEffect(() => {
    try {
      // Limit history size to prevent memory issues
      const limitedHistory = transcriptionHistory.slice(-maxHistorySize);
      localStorage.setItem('voxbridge_conversation_history', JSON.stringify(limitedHistory));
      if (limitedHistory.length > 0) {
        console.log(`üíæ Saved ${limitedHistory.length} messages to history`);
      }
    } catch (error) {
      console.warn('Failed to save conversation history:', error);
    }
  }, [transcriptionHistory]);

  /**
   * Translate text using Indian Language AI
   */
  const translateWithIndianAI = useCallback(async (text: string, sourceLang: Language, targetLang: Language): Promise<string> => {
    const sourceCode = sourceLang.code.toLowerCase().split('-')[0];
    const targetCode = targetLang.code.toLowerCase().split('-')[0];
    
    console.log(`üîÑ TRANSLATION REQUEST:`);
    console.log(`   Input Text: "${text}"`);
    console.log(`   Source: ${sourceLang.name} (${sourceLang.code}) ‚Üí Code: ${sourceCode}`);
    console.log(`   Target: ${targetLang.name} (${targetLang.code}) ‚Üí Code: ${targetCode}`);
    
    if (!text || text.trim().length === 0) {
      return '';
    }

    try {
      const translation = await indianAI.current.translateIndianLanguages(text, sourceCode, targetCode);
      console.log(`‚úÖ TRANSLATION RESULT: "${translation}"`);
      console.log(`   Original: "${text}" (${sourceCode})`);
      console.log(`   Translated: "${translation}" (${targetCode})`);
      return translation;
    } catch (error) {
      console.error('‚ùå Translation failed:', error);
      return `[${targetCode.toUpperCase()}] ${text}`;
    }
  }, []);

  /**
   * Speak translated text using TTS (non-blocking)
   */
  const speakTranslation = useCallback(async (text: string, language: Language): Promise<void> => {
    if (!text || text.trim().length === 0) return;

    try {
      // Don't await - start TTS and return immediately!
      indianAI.current.speakWithIndianContext(text, language.code.toLowerCase().split('-')[0]);
      
      // Return immediately - don't wait for speech!
      return Promise.resolve();
    } catch (error) {
      console.error('‚ùå Speech failed:', error);
      return Promise.resolve(); // Don't throw
    }
  }, []);

  /**
   * End session and cleanup - Clears conversation for new session
   */
  const endSession = useCallback(() => {
    console.log('üõë Ending session and starting new conversation...');
    
    // Reset restart counter
    restartAttemptsRef.current = 0;
    
    // Clean up recognition properly
    if (recognitionRef.current) {
      try {
        recognitionRef.current.onend = null;
        recognitionRef.current.onresult = null;
        recognitionRef.current.onerror = null;
        recognitionRef.current.stop();
      } catch (e) {
        console.warn('Error stopping recognition:', e);
      }
      recognitionRef.current = null;
    }

    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }

    if (audioContextRef.current) {
      try {
        audioContextRef.current.close();
      } catch (e) {
        console.warn('Error closing audio context:', e);
      }
      audioContextRef.current = null;
    }

    // Stop any ongoing speech
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel();
    }

    isListeningRef.current = false;
    isProcessingRef.current = false;
    
    // Clear current conversation for new session
    setTranscriptionHistory([]);
    setCurrentInputTranscription('');
    setCurrentOutputTranscription('');
    
    setStatus(SessionStatus.DISCONNECTED);
    console.log('‚úÖ Session ended - Ready for new conversation');
  }, []);

  /**
   * Start translation session
   */
  const startSession = useCallback(async (sourceLang: Language, targetLang: Language) => {
    setStatus(SessionStatus.CONNECTING);
    setError(null);
    
    // Reset restart counter
    restartAttemptsRef.current = 0;
    
    // Only clear history if starting fresh (not restarting)
    if (recognitionRef.current === null) {
      setTranscriptionHistory([]);
    }
    setCurrentInputTranscription('');
    setCurrentOutputTranscription('');
    isProcessingRef.current = false;
    
    // Clean up existing recognition if recreating
    if (recognitionRef.current) {
      try {
        recognitionRef.current.onend = null;
        recognitionRef.current.onresult = null;
        recognitionRef.current.onerror = null;
        recognitionRef.current.stop();
      } catch (e) {
        // Ignore errors during cleanup
      }
      recognitionRef.current = null;
    }

    try {
      // Preload voices for TTS
      console.log('üé§ Preloading voices...');
      if ('speechSynthesis' in window) {
        window.speechSynthesis.getVoices();
        if (window.speechSynthesis.getVoices().length === 0) {
          await new Promise(resolve => {
            window.speechSynthesis.onvoiceschanged = () => {
              console.log('‚úÖ Voices loaded');
              resolve(undefined);
            };
            setTimeout(resolve, 2000);
          });
        }
      }

      // Setup microphone for visualization
      try {
        mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
        const source = audioContextRef.current.createMediaStreamSource(mediaStreamRef.current);
        analyserNodeRef.current = audioContextRef.current.createAnalyser();
        analyserNodeRef.current.fftSize = 256;
        source.connect(analyserNodeRef.current);
      } catch (micError) {
        console.warn('‚ö†Ô∏è Microphone access limited');
      }

      // Setup speech recognition
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      if (!SpeechRecognition) {
        throw new Error('Speech recognition not supported. Please use Chrome or Edge.');
      }

      recognitionRef.current = new SpeechRecognition();
      recognitionRef.current.continuous = true;
      recognitionRef.current.interimResults = true;
      recognitionRef.current.lang = sourceLang.code;
      recognitionRef.current.maxAlternatives = 5; // More alternatives for better accuracy
      
      // Improve recognition accuracy
      if ('grammars' in recognitionRef.current) {
        // Enable grammar if available
        try {
          const grammar = '#JSGF V1.0; grammar names; public <name> = Aakash | Akash | Raj | Ram | Rahul;';
          const speechRecognitionList = new (window as any).webkitSpeechGrammarList || new (window as any).SpeechGrammarList();
          speechRecognitionList.addFromString(grammar, 1);
          recognitionRef.current.grammars = speechRecognitionList;
        } catch (e) {
          console.log('Grammar not supported, using default recognition');
        }
      }

      let lastProcessedText = '';
      let processingTimeout: NodeJS.Timeout | null = null;

      recognitionRef.current.onresult = async (event: any) => {
        let interimTranscript = '';
        let newFinalTranscript = '';

          // Process all results including alternatives for better accuracy
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          
          // Use the best alternative (highest confidence)
          let bestAlternative = result[0];
          let bestConfidence = result[0].confidence || 0;
          
          // Check all alternatives and pick the best one
          for (let j = 0; j < Math.min(result.length, 5); j++) {
            const alt = result[j];
            const altConfidence = alt.confidence || 0;
            if (altConfidence > bestConfidence) {
              bestAlternative = alt;
              bestConfidence = altConfidence;
            }
          }
          
          const transcript = bestAlternative.transcript;
          const confidence = bestConfidence;
          
          // Log if we used a different alternative
          if (result.length > 1 && bestAlternative !== result[0]) {
            console.log(`üéØ Using best alternative: "${transcript}" (${(confidence * 100).toFixed(1)}%) instead of "${result[0].transcript}" (${((result[0].confidence || 0) * 100).toFixed(1)}%)`);
          }
          
          if (result.isFinal) {
            newFinalTranscript += transcript;
            console.log(`‚úÖ Final: "${transcript}" (confidence: ${(confidence * 100).toFixed(1)}%)`);
            
            // Skip if confidence is too low (likely wrong recognition)
            if (confidence < 0.3) {
              console.warn(`‚ö†Ô∏è Very low confidence (${(confidence * 100).toFixed(1)}%) - Skipping this result`);
              continue; // Skip this result
            }
          } else {
            interimTranscript += transcript;
          }
        }

        // Show current speech (only interim for preview)
        setCurrentInputTranscription(interimTranscript);

        // Clear any pending processing timeout
        if (processingTimeout) {
          clearTimeout(processingTimeout);
        }

        // Process final transcript immediately (no accumulation)
        if (newFinalTranscript.trim() && !isProcessingRef.current) {
          const rawText = newFinalTranscript.trim();
          
          // Only process if it's new text and meaningful length
          if (rawText !== lastProcessedText && rawText.length > 1) {
            lastProcessedText = rawText;
            
            // Process immediately - FAST MODE!
            isProcessingRef.current = true;
            console.log(`üß† Processing: "${rawText}"`);
            
            // Clear input immediately for next sentence
            setCurrentInputTranscription('');
            
            // Process translation asynchronously (don't block!)
            (async () => {
              try {
                setCurrentOutputTranscription('Translating...'); // Show loading
                
                // Translate using Indian Language AI
                const translation = await translateWithIndianAI(rawText, sourceLang, targetLang);
                
                setCurrentOutputTranscription(translation);
                
                // Add to history (with size limit to prevent memory issues)
                setTranscriptionHistory(prev => {
                  const newEntries = [];

                  // Add user input
                  const lastUserEntry = prev.filter(entry => entry.speaker === 'user').pop();
                  if (!lastUserEntry || lastUserEntry.text !== rawText) {
                    newEntries.push({ speaker: 'user', text: rawText });
                  }

                  // Add translation
                  const lastModelEntry = prev.filter(entry => entry.speaker === 'model').pop();
                  if (!lastModelEntry || lastModelEntry.text !== translation) {
                    newEntries.push({ speaker: 'model', text: translation });
                  }

                  const updated = [...prev, ...newEntries];
                  // Limit history size to prevent memory overload
                  return updated.slice(-maxHistorySize);
                });

                // Speak translation using TTS (NON-BLOCKING - don't wait!)
                // Process next sentence immediately without waiting for TTS
                console.log(`üé§ Speaking: "${translation}" (non-blocking)`);
                if (translation && translation.trim()) {
                  // Don't await - let it play in background!
                  speakTranslation(translation, targetLang).catch(err => {
                    console.error('TTS error (non-critical):', err);
                  });
                }

                // Clear output after 2 seconds (fast!)
                setTimeout(() => {
                  setCurrentOutputTranscription('');
                }, 2000);

              } catch (error) {
                console.error('‚ùå Process error:', error);
                setCurrentOutputTranscription('‚ùå Translation failed');
                setTimeout(() => {
                  setCurrentOutputTranscription('');
                }, 2000);
              }

              // Allow next sentence immediately!
              isProcessingRef.current = false;
            })();
            
            // Don't wait - allow next sentence processing right away!
            // isProcessingRef will be set to false inside async function
          }
        }
      };

      recognitionRef.current.onerror = (event: any) => {
        console.error('‚ùå Recognition error:', event.error);
        
        // Reset restart counter on error
        restartAttemptsRef.current = 0;
        
        if (event.error === 'not-allowed') {
          setError('Microphone access denied. Please allow microphone access.');
          isListeningRef.current = false;
        } else if (event.error === 'audio-capture') {
          setError('No microphone found. Please connect a microphone.');
          isListeningRef.current = false;
        } else if (event.error === 'network') {
          console.warn('‚ö†Ô∏è Network error - will retry on next restart');
          // Don't stop listening for network errors, let it retry
        } else if (event.error === 'no-speech') {
          // This is normal, just silence - ignore it
          console.log('‚ÑπÔ∏è No speech detected (normal)');
        } else if (event.error === 'aborted') {
          // Recognition was stopped - normal, ignore
          console.log('‚ÑπÔ∏è Recognition aborted (normal)');
        } else {
          console.warn(`‚ö†Ô∏è Recognition error: ${event.error} - will retry`);
          // For other errors, try to continue
          if (isListeningRef.current && status === SessionStatus.CONNECTED) {
            setTimeout(() => {
              if (recognitionRef.current && isListeningRef.current) {
                try {
                  recognitionRef.current.start();
                } catch (e) {
                  console.error('Failed to restart after error:', e);
                }
              }
            }, 100);
          }
        }
      };

      recognitionRef.current.onend = () => {
        console.log(`üé§ Recognition ended (restart attempts: ${restartAttemptsRef.current})`);
        
        if (!isListeningRef.current || status !== SessionStatus.CONNECTED) {
          restartAttemptsRef.current = 0; // Reset counter if not listening
          return;
        }

        // If too many restart attempts, recreate recognition instance to prevent memory issues
        if (restartAttemptsRef.current >= maxRestartAttempts) {
          console.warn(`‚ö†Ô∏è Too many restart attempts (${restartAttemptsRef.current}), recreating recognition...`);
          restartAttemptsRef.current = 0;
          
          // Clean up old recognition
          try {
            if (recognitionRef.current) {
              recognitionRef.current.onend = null;
              recognitionRef.current.onresult = null;
              recognitionRef.current.onerror = null;
              recognitionRef.current.stop();
            }
          } catch (e) {
            console.warn('Cleanup error:', e);
          }

          // Recreate recognition after delay
          setTimeout(() => {
            if (isListeningRef.current && status === SessionStatus.CONNECTED) {
              console.log('üîÑ Recreating recognition instance...');
              
              // Clean up completely
              recognitionRef.current = null;
              
              // Recreate recognition instance
              const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
              if (SpeechRecognition) {
                recognitionRef.current = new SpeechRecognition();
                recognitionRef.current.continuous = true;
                recognitionRef.current.interimResults = true;
                recognitionRef.current.lang = sourceLang.code;
                recognitionRef.current.maxAlternatives = 5;
                
                // Reattach handlers (need to recreate handlers with current scope)
                // This will be done by re-initializing the handlers
                console.log('‚ö†Ô∏è Please restart session manually to recreate recognition handlers');
                setError('Recognition instance recreated. Please click Start again.');
                isListeningRef.current = false;
              }
            }
          }, 200);
          return;
        }

        // Normal restart
        restartAttemptsRef.current++;
        setTimeout(() => {
          try {
            if (recognitionRef.current && isListeningRef.current && status === SessionStatus.CONNECTED) {
              console.log(`üîÑ Restarting recognition (attempt ${restartAttemptsRef.current})...`);
              recognitionRef.current.start();
              restartAttemptsRef.current = 0; // Reset on successful restart
            }
          } catch (error) {
            console.error(`‚ùå Restart failed (attempt ${restartAttemptsRef.current}):`, error);
            // Try again after short delay
            setTimeout(() => {
              if (recognitionRef.current && isListeningRef.current && status === SessionStatus.CONNECTED) {
                try {
                  recognitionRef.current.start();
                  restartAttemptsRef.current = 0; // Reset on successful restart
                } catch (e) {
                  console.error(`‚ùå Second restart attempt failed:`, e);
                  // Don't increment counter here, will be handled by next onend
                }
              }
            }, 100);
          }
        }, 50); // Faster restart
      };

      // Start listening
      isListeningRef.current = true;
      recognitionRef.current.start();
      setStatus(SessionStatus.CONNECTED);
      console.log('‚úÖ Real-time translator started');

    } catch (error) {
      console.error('‚ùå Start failed:', error);
      endSession();
      setError(error instanceof Error ? error.message : 'Failed to start session');
      setStatus(SessionStatus.ERROR);
    }
  }, [endSession, translateWithIndianAI, speakTranslation, status]);

  const pauseSession = useCallback(() => {
    console.log('‚è∏Ô∏è Pausing session...');
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
    isListeningRef.current = false;
    setStatus(SessionStatus.PAUSED);
    console.log('‚úÖ Session paused');
  }, []);

  const resumeSession = useCallback(() => {
    console.log('‚ñ∂Ô∏è Resuming session...');
    if (recognitionRef.current && status === SessionStatus.PAUSED) {
      setStatus(SessionStatus.CONNECTED);
      isListeningRef.current = true;
      try {
        recognitionRef.current.start();
        console.log('‚úÖ Session resumed');
      } catch (error) {
        console.error('‚ùå Resume failed:', error);
        setError('Failed to resume recognition');
      }
    }
  }, [status]);

  const clearError = useCallback(() => {
    setError(null);
    if (status === SessionStatus.ERROR) {
      setStatus(SessionStatus.DISCONNECTED);
    }
  }, [status]);

  const clearHistory = useCallback(() => {
    console.log('üóëÔ∏è Clearing conversation history...');
    setTranscriptionHistory([]);
    setCurrentInputTranscription('');
    setCurrentOutputTranscription('');
    // Also clear from localStorage
    localStorage.removeItem('voxbridge_conversation_history');
    console.log('‚úÖ Conversation history cleared');
  }, []);

  const getIndianAIStats = useCallback(() => {
    const indianStats = indianAI.current.getIndianAIStats();
    
    return {
      indianLanguages: {
        supportedLanguages: indianStats.supportedLanguages,
        features: indianStats.features,
        status: indianStats.status,
        voicesLoaded: indianStats.voicesLoaded,
        cachedVoices: indianStats.cachedVoices
      },
      overall: {
        indianLanguageOptimized: true,
        translationAccuracy: 88,
        gttsEnabled: true
      }
    };
  }, []);

  return {
    status,
    error,
    startSession,
    endSession,
    pauseSession,
    resumeSession,
    clearError,
    clearHistory,
    transcriptionHistory,
    currentInputTranscription,
    currentOutputTranscription,
    analyserNode: analyserNodeRef.current,
    getIndianAIStats,
    translateWithIndianAI
  };
};
